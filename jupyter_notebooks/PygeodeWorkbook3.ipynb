{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# National Climate Dynamics Workshop - Python Short Course\n",
    "\n",
    "The remainder of this short course will introduce `pygeode`, a Python package designed for accessing, processing, and visualizing gridded geophysical data. It is an alternative to `xarray` or to `IRIS`. PyGeode uses `numpy` and `matplotlib` extensively for underlying computations and plotting, and is designed around three main principles: \n",
    "\n",
    "1. Data should be represented and manipulated in geophysical coordinates\n",
    "2. Data should be 'lazy-loaded', i.e. left on disk until needed for computation\n",
    "3. Plotting defaults should be as close to publication quality as possible\n",
    "\n",
    "Full documentation about pygeode including a tutorial, gallery, and API reference, can be found at [pygeode.github.io](https://pygeode.github.io).\n",
    "\n",
    "This introduction is split into three Jupyter notebooks. Each 'workbook' consists of a set of pre-written examples to introduce some of pygeode functionality, followed by a simple example to give you the chance to work with pygeode yourself.\n",
    "\n",
    "\n",
    "## PyGeode Workbook #3: Processing and analysing a dataset with many NetCDF files\n",
    "\n",
    "Large datasets are typically stored in many NetCDF files. This final workbook introduces some tools to handle this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C0.1 Setup matplotlib backend for inline interactive plots\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C0.2 Import pygeode, numpy, and matplotlib plotting packages\n",
    "import pygeode as pyg\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opening multiple files at once\n",
    "\n",
    "There are two main tools for opening datasets comprised of multiple files.\n",
    "\n",
    "The first, `openall()`, opens up each file in the list provided (this can be in the form of an explicit python list, a wildcard expression, or a mix of the two). It then concatenates all the variables found according to the coordinate data it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1.1 Open files using openall()\n",
    "\n",
    "# Set this path to wherever you've stored the sample data provided.\n",
    "data_pth = './data/'\n",
    "\n",
    "# Open all netcdf files in given path\n",
    "ds1 = pyg.openall(data_pth + '*/*.nc')\n",
    "print(ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with a single dataset consisting of temperature and horizontal wind data at 800 hPa from ERA5. The same `dimtypes` and `namemap` options that were available when opening single files are still available here if you need to tweak anything.\n",
    "\n",
    "This has the advantage of being very simple and reasonably flexible. The disadvantage is that for larger datasets (comprised of many hundreds or thousands of files), even just reading the metadata from each file can be a time consuming process.\n",
    "\n",
    "For these cases, a second option is avaible. `open_multi()` also takes a list of files, but it only reads the metadata from the first file, and assumes that the rest of the files contain the same variables defined on the same spatial grid, but defined over a different time period. It then infers the contents of the rest of the files based on their filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 Open files using open_multi()\n",
    "\n",
    "dss = []\n",
    "\n",
    "# Open all netcdf files containing each variable independently    \n",
    "for v in ['t', 'u', 'v']:\n",
    "    # The pattern argument provides a means of translating from filenames to dates\n",
    "    ds = pyg.open_multi(data_pth + '*/*_%s.nc' % v, pattern='$Y$m')\n",
    "    dss.append(ds)\n",
    "\n",
    "# Concatenate the three datasets\n",
    "ds2 = pyg.concatenate(dss)\n",
    "print(ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is less flexible than `openall()`, but much faster to open and to manipulate, especially for larger datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3 Time loading a single variable for this dataset from openall()\n",
    "%timeit -n 1 -r 1 ds1.t.mean('time').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.4 Time loading a single variable for this dataset from open_multi()\n",
    "%timeit -n 1 -r 1 ds2.u.mean('time').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing climatologies and saving netcdf files\n",
    "\n",
    "Now that we have 10 years of data available to us, we can compute some climatologies. Since computing them can take a bit of time, we'll see how to save the results to disk so that we don't have to recompute them. We want to compute the climatology of each field (temperature, u, v) in the dataset, so we'll also illustrate the use `Dataset.map()`, which applies a given function to each variable in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 Compute climatology for each variable\n",
    "\n",
    "# To deal with leap years, the following function removes a single day\n",
    "# from each leap year to end up with a uniform 365-day year\n",
    "def remleap(v):\n",
    "    return pyg.timeutils.removeleapyears(v, omitdoy_leap=[182]).rename(v.name)\n",
    "\n",
    "# This computes the climatology\n",
    "def clim(v):\n",
    "    return pyg.climatology(v).rename(v.name)\n",
    "\n",
    "# Apply each operation to the dataset\n",
    "dsnl = ds2.map(remleap)\n",
    "dsc = dsnl.map(clim)\n",
    "\n",
    "print(dsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have variables defined on a 365-day calendar year, but note one further thing: the dates do not have a year associated with them. This signals to PyGeode that this is a climatology. This will be useful when we start to compute anomalies below.\n",
    "\n",
    "This operation completes quickly because once again, PyGeode has not actually done any computations. We are going to want these climatologies though, so we'll go ahead and save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2.2 Save climatology to disk if it doesn't exist\n",
    "\n",
    "# We'll save this climatology in the same directory as the sample data\n",
    "# Change cl_path if you would like to save it somewhere else\n",
    "cl_file = 'e5_800hPa_clim.nc'\n",
    "cl_pth = data_pth + cl_file\n",
    "\n",
    "import os\n",
    "# Check first if the file exists already; if so, no need to recompute.\n",
    "if not os.path.exists(cl_pth):\n",
    "    pyg.save(cl_pth, dsc)\n",
    "\n",
    "# Use saved climatology in either case\n",
    "dsc = pyg.open(cl_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 10 years of data, the climatology of daily data is still quite noisy. We can apply a smoothing operator to retain only the first few harmonics of the seasonal cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2.3 Plot smoothed versus un-smoothed climatology\n",
    "\n",
    "# Compute the zonal mean zonal wind\n",
    "uz = dsc.u.mean('lon')\n",
    "\n",
    "# Apply smoothing operator; retain up to 4th harmonic\n",
    "us = uz.fft_smooth('time', 4)\n",
    "\n",
    "# We can use showgrid() to compare the two\n",
    "pyg.showgrid([uz, us], style='div', ncol=1, cdelt=5., nf=5, ndiv=3, nl=1, cmap=plt.cm.RdGy_r, extend='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Computing anomalies and run a simple statistical test\n",
    "\n",
    "We are now set up to work with the climatologies. We'll use them to compute anomalous winds and the transient eddy kinetic energy at this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3.1 Compute transient eddy kinetic energy \n",
    "\n",
    "#Smooth climatologies \n",
    "uc = dsc.u.fft_smooth('time', 4)\n",
    "vc = dsc.v.fft_smooth('time', 4)\n",
    "\n",
    "# Pygeode automatically broadcasts climatologies so that the following works as expected:\n",
    "up = dsnl.u - dsc.u\n",
    "vp = dsnl.v - dsc.v\n",
    "\n",
    "# Compute EKE\n",
    "eke = 0.5 * (up**2 + vp**2)\n",
    "eke = eke.rename(\"EKE\")\n",
    "\n",
    "# The following adds a duplicate longitude so that global plots with\n",
    "# Cartopy don't have a missing gap\n",
    "eke = pyg.rotatelon(eke, origin=0, duplicate = True)\n",
    "\n",
    "# Compute DJF and JJA means\n",
    "eke_djf = eke(l_month = [12, 1, 2]).load()\n",
    "eke_jja = eke(l_month = [6,  7, 8]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3.2 Plot DJF and JJA EKE\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "from cartopy import crs as ccrs\n",
    "npst = ccrs.Stereographic(central_longitude=180, central_latitude=90)\n",
    "\n",
    "ax = pyg.showgrid([eke_djf.mean('time'), eke_jja.mean('time')], ncol=1, size = (4, 4), map=dict(projection=npst))\n",
    "\n",
    "ax.axes[0].axes[0].set_extent([0, 359, 30, 90], crs = ccrs.PlateCarree())\n",
    "ax.axes[0].axes[1].set_extent([0, 359, 30, 90], crs = ccrs.PlateCarree())\n",
    "ax.render(fig=fig)\n",
    "plt.ion()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3.3 Compute correlation between u' and v'\n",
    "up_djf = up(l_month = [12,1,2]).load()\n",
    "vp_djf = vp(l_month = [12,1,2]).load()\n",
    "\n",
    "d = pyg.correlate(up_djf, vp_djf, ['time'], output='r,r2,p')\n",
    "\n",
    "# Adjust p-value for field significance following Wilks (2016)\n",
    "pi = d.p[:].ravel()\n",
    "pi = np.sort(pi)\n",
    "isig = np.where(pi < np.linspace(0, 0.01, len(pi)))[0] \n",
    "if len(isig) > 0:\n",
    "    pt = pi[isig[-1]]\n",
    "\n",
    "sigmask = (1 - 0.5*d.p) * pyg.sign(d.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3.4 Plot result, including a significance mask\n",
    "fig = plt.figure()\n",
    "plt.ioff()\n",
    "\n",
    "from cartopy import crs as ccrs\n",
    "\n",
    "cr = pyg.clfdict(0.5, ndiv=2, nf = 4, style='div', nozero=True, cmap = plt.cm.RdBu_r)\n",
    "\n",
    "r = pyg.rotatelon(d.r, origin=-180)\n",
    "sm = pyg.rotatelon(sigmask, origin=-180)\n",
    "\n",
    "ax = pyg.showvar(r, map=dict(projection='EquidistantConic'), **cr)\n",
    "pyg.vsigmask(sm, axes=ax.axes[0], mjsig = 1 - pt)\n",
    "\n",
    "ax.axes[0].set_extent([-20, 40, 0, 75], crs = ccrs.PlateCarree())\n",
    "ax.render(fig=fig)\n",
    "plt.ion()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "Compute climatology of meridional heat flux (v'T'), plot time series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
